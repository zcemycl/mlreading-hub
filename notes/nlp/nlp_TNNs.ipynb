{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 761/8544 [00:00<00:01, 7606.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading Embeddings ... Done\n",
      "Vocabulary size:  400000\n",
      "Embeddings:  [0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8544/8544 [00:01<00:00, 4306.86it/s]\n",
      "100%|██████████| 2210/2210 [00:00<00:00, 7943.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies required in Tree:  18646\n",
      "----------\n",
      "Embedding Matrix:  [[-0.019331  -0.98478   -0.090353  ...  0.5815    -0.56574   -0.54883  ]\n",
      " [-1.0163     0.2053    -0.013379  ...  0.81246   -0.36699    0.62712  ]\n",
      " [ 0.40235   -0.45165   -0.90334   ...  0.68559    0.97844    0.36966  ]\n",
      " ...\n",
      " [-0.72956    0.38623    1.0412    ... -1.2386    -0.86671    0.46243  ]\n",
      " [-0.73887   -0.88399   -0.0055762 ... -0.91796   -0.31749   -1.2714   ]\n",
      " [-0.22895    0.30128    1.523     ... -0.1907     0.42533    0.2329   ]]\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from heapq import heappush,heappop\n",
    "from collections import Counter,defaultdict\n",
    "import multiprocessing as mp\n",
    "from sklearn.utils import shuffle\n",
    "import time\n",
    "import networkx as nx\n",
    "import copy\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "print(\"Is GPU available? \",torch.cuda.is_available())\n",
    "\n",
    "def preOrder(tree):\n",
    "    levels = dict()\n",
    "    def dfs(root,level,i):\n",
    "        if root==None:\n",
    "            return i\n",
    "        ls = levels.get(level,[])\n",
    "        ls.append(root.word+\":\"+str(i))\n",
    "        i+=1\n",
    "        levels[level]=ls\n",
    "        i = dfs(root.left,level+1,i)\n",
    "        i = dfs(root.right,level+1,i)\n",
    "        return i\n",
    "    dfs(tree,0,0)\n",
    "    return levels\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self,left=None,right=None,word=\"\"):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.word = word\n",
    "\n",
    "class Solution:\n",
    "    def solve(self,string):\n",
    "        self.i=0\n",
    "        self.word=\"\"\n",
    "        tree = Tree()\n",
    "        def dfs(root): # preorder\n",
    "            if self.i==len(string):\n",
    "                return\n",
    "            if string[self.i]==\"(\":\n",
    "                if self.word.strip()!=\"\":\n",
    "                    root.word=self.word.strip()\n",
    "                else:\n",
    "                    self.i+=1\n",
    "                    return dfs(root)\n",
    "                self.word=\"\"\n",
    "                self.i+=1\n",
    "                root.left=Tree()\n",
    "                dfs(root.left)\n",
    "                while string[self.i]==\")\":\n",
    "                    self.i+=1\n",
    "                root.right=Tree()\n",
    "                dfs(root.right)\n",
    "            elif string[self.i]==\")\":\n",
    "                root.word=self.word.strip()\n",
    "                self.word=\"\"\n",
    "                self.i+=1\n",
    "            else:\n",
    "                self.word+=string[self.i]\n",
    "                self.i+=1\n",
    "                dfs(root)\n",
    "        dfs(tree)\n",
    "        return tree\n",
    "    \n",
    "def draw(levels):\n",
    "    levels_ = copy.deepcopy(levels)\n",
    "    stack = [levels_[0][0]]\n",
    "    G = nx.Graph()\n",
    "    i = 1\n",
    "    while stack:\n",
    "        prev = stack.pop(0)\n",
    "        if i not in levels_:\n",
    "            break\n",
    "        cur1 = levels_[i].pop(0)\n",
    "        cur2 = levels_[i].pop(0)\n",
    "        if cur1.split(\":\")[0].strip() in string.digits:\n",
    "            stack.append(cur1)\n",
    "        if cur2.split(\":\")[0].strip() in string.digits:\n",
    "            stack.append(cur2)\n",
    "        G.add_edge(prev,cur1)\n",
    "        G.add_edge(prev,cur2)\n",
    "        if len(levels_[i])==0:\n",
    "            i+=1\n",
    "    pos = {}\n",
    "    for key in levels.keys():\n",
    "        width = len(levels[key])\n",
    "        x0 = -0.5-(width//2-1)\n",
    "        for j in range(len(levels[key])):\n",
    "            try:\n",
    "                x = int(levels[key][j].split(\":\")[1])\n",
    "                pos[levels[key][j]]=(x0+j,10-key*0.5)\n",
    "            except:\n",
    "                pass\n",
    "    d = dict(G.degree)\n",
    "    plt.figure(figsize=(20,20*len(levels)/8))\n",
    "    nx.draw_networkx(G,pos,\n",
    "        node_size=[5000 for v in d.values()])\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "s = Solution()\n",
    "        \n",
    "PATHglove=\"/home/yui/Documents/data/nlp/glove.6B/glove.6B.50d.txt\"\n",
    "PATH_Train_Tree=\"/home/yui/Documents/data/nlp/trainDevTestTrees_PTB/trees/train.txt\"\n",
    "PATH_Test_Tree=\"/home/yui/Documents/data/nlp/trainDevTestTrees_PTB/trees/test.txt\"\n",
    "\n",
    "class preprocess:\n",
    "    def __init__(self,PATHglove,PATH_Train_Tree,\n",
    "                PATH_Test_Tree):\n",
    "        self.PATHglove = PATHglove\n",
    "        self.PATH_Train_Tree=PATH_Train_Tree\n",
    "        self.PATH_Test_Tree=PATH_Test_Tree\n",
    "        self.l2id,self.id2l=dict(),dict()\n",
    "        self.initEmbeddings()\n",
    "        self.loadTreeString()\n",
    "        self.getRequiredVocabs()\n",
    "        self.buildEmbeddingsMatrix()\n",
    "    def initEmbeddings(self):\n",
    "        self.w2id,self.id2w,self.res={},{},[]\n",
    "        with open(self.PATHglove) as f:\n",
    "            lines = f.readlines()\n",
    "            for i,line in enumerate(lines):\n",
    "                line = line.strip()\n",
    "                tokens = line.split(\" \")\n",
    "                word = tokens[0]\n",
    "                vec = list(map(float,tokens[1:]))\n",
    "                self.res.append(vec)\n",
    "                self.w2id[word]=i\n",
    "                self.id2w[i]=word\n",
    "        self.k = len(self.res[0])\n",
    "        print(\"[INFO] Loading Embeddings ... Done\")\n",
    "        print(\"Vocabulary size: \",len(self.w2id))\n",
    "        print(\"Embeddings: \",self.res[0])\n",
    "        print(\"----------\")\n",
    "    def loadTreeString(self):\n",
    "        self.data = []\n",
    "        with open(self.PATH_Train_Tree) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                self.data.append(line)\n",
    "        self.tdata = []\n",
    "        with open(self.PATH_Test_Tree) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                self.tdata.append(line)\n",
    "    def extractLevels(self,l):\n",
    "        words,j = [],0\n",
    "        for key in l.keys():\n",
    "            ls = l[key]\n",
    "            for ele in ls:\n",
    "                partition=ele.split(\":\")[0].strip().split(\" \")\n",
    "                label = partition[0].strip()\n",
    "                if label not in self.l2id:\n",
    "                    self.l2id[label]=j\n",
    "                    self.id2l[j]=label\n",
    "                    j+=1\n",
    "                if len(partition)==2:\n",
    "                    word = partition[1].lower()\n",
    "                    self.vocabs.add(word)\n",
    "                    words.append(word)\n",
    "                    if word not in self.w2id:\n",
    "                        i=len(self.w2id)\n",
    "                        self.w2id[word]=i\n",
    "                        self.id2w[i]=word\n",
    "                \n",
    "        return words\n",
    "    def getRequiredVocabs(self):\n",
    "        self.trainTrees,self.testTrees=[],[]\n",
    "        self.vocabs=set()\n",
    "        s = Solution()\n",
    "        for d in tqdm(self.data):\n",
    "            t = s.solve(d)\n",
    "            self.trainTrees.append(t)\n",
    "            l = preOrder(t)\n",
    "            words = self.extractLevels(l)\n",
    "        for d in tqdm(self.tdata):\n",
    "            t = s.solve(d)\n",
    "            self.testTrees.append(t)\n",
    "            l = preOrder(t)\n",
    "            words = self.extractLevels(l)\n",
    "            \n",
    "        print(\"Vocabularies required in Tree: \",len(self.vocabs))\n",
    "        print(\"----------\")\n",
    "        \n",
    "    def buildEmbeddingsMatrix(self):\n",
    "        self.embedMatrix=np.zeros((len(self.vocabs),self.k))\n",
    "        self.w2id_exp,self.id2w_exp=dict(),dict()\n",
    "        for i,vocab in enumerate(self.vocabs):\n",
    "            self.w2id_exp[vocab]=i\n",
    "            self.id2w_exp[i]=vocab\n",
    "            try:\n",
    "                self.embedMatrix[i]=self.res[self.w2id[vocab]]\n",
    "            except:\n",
    "                self.embedMatrix[i]=np.random.randn(self.k)\n",
    "        print(\"Embedding Matrix: \",self.embedMatrix)\n",
    "        print(\"----------\")\n",
    "pp = preprocess(PATHglove,PATH_Train_Tree,PATH_Test_Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TNN(nn.Module):\n",
    "    def __init__(self,pp):\n",
    "        super().__init__()\n",
    "        self.pp = pp\n",
    "        self.emb = nn.Embedding(len(pp.w2id_exp),\n",
    "                        len(pp.res[0]))\n",
    "        self.emb.load_state_dict({'weight':\\\n",
    "                        torch.Tensor(pp.embedMatrix)})\n",
    "        self.emb.weight.requires_grad = False\n",
    "        self.Wl = nn.parameter.Parameter(\\\n",
    "            torch.randn(len(pp.res[0]),len(pp.res[0])))\n",
    "        self.Wr= nn.parameter.Parameter(\\\n",
    "            torch.randn(len(pp.res[0]),len(pp.res[0])))\n",
    "        self.b = nn.parameter.Parameter(\\\n",
    "            torch.randn(len(pp.res[0])))\n",
    "        self.Wo = nn.parameter.Parameter(\\\n",
    "            torch.randn(len(pp.res[0]),len(pp.l2id)))\n",
    "        self.bo = nn.parameter.Parameter(\\\n",
    "            torch.randn(len(pp.l2id)))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.outF = nn.Softmax(dim=2)\n",
    "        self.actF = nn.ReLU()\n",
    "    def postOrder(self,tree):\n",
    "        def dfs(root):\n",
    "            if root.word.strip() not in string.digits:\n",
    "                tokens = root.word.strip().split(\" \")\n",
    "                score,word=tokens[0],tokens[1].lower()\n",
    "                v=torch.tensor([[self.pp.w2id_exp[word]]])\n",
    "                v=self.emb(v)\n",
    "                out=self.outF(torch.matmul(v,\n",
    "                    self.Wo)+self.bo).squeeze(0)\n",
    "                target=torch.tensor([self.pp.l2id[score]])\n",
    "                return v,self.loss(out,target),\\\n",
    "                    1*(torch.argmax(out)==target.squeeze())\n",
    "            xl,lossl,hitl=dfs(root.left)\n",
    "            xr,lossr,hitr= dfs(root.right)\n",
    "            h = torch.matmul(xl,self.Wl)+self.b+\\\n",
    "                    torch.matmul(xr,self.Wr)\n",
    "            h = self.actF(h)\n",
    "            tokens = root.word.strip().split(\" \")\n",
    "            out=self.outF(torch.matmul(h,\n",
    "                self.Wo)+self.bo).squeeze(0)\n",
    "            target=torch.tensor([self.pp.l2id[tokens[0]]])\n",
    "            return h,self.loss(out,target),\\\n",
    "                1*(torch.argmax(out)==target.squeeze())\n",
    "        return dfs(tree)\n",
    "    def forward(self,x):\n",
    "        if type(x)==Tree:\n",
    "            return self.postOrder(x)\n",
    "        elif type(x)==list:\n",
    "            h,loss = torch.empty((len(x),1,self.pp.k)),0\n",
    "            hit,ignore=0,[]\n",
    "            for i,x_ in enumerate(x):\n",
    "                try:\n",
    "                    h[i],loss_,hit_=self.postOrder(x_)\n",
    "                    hit+=hit_\n",
    "                    loss+=loss_\n",
    "                except:\n",
    "                    ignore.append(i)\n",
    "            return h,loss,hit,(len(x)-len(ignore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:24<00:00,  2.65it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 1.6453\n",
      "Epoch 0 training accuracy: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:25<00:00,  2.62it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 training loss: 1.6464\n",
      "Epoch 1 training accuracy: 0.2584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:24<00:00,  2.65it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 training loss: 1.6457\n",
      "Epoch 2 training accuracy: 0.2591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 66/66 [00:25<00:00,  2.63it/s]\n",
      "  0%|          | 0/66 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 training loss: 1.6463\n",
      "Epoch 3 training accuracy: 0.2585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 10/66 [00:04<00:23,  2.39it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1edff8b4b99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         _,loss,hits,cases=model(\\\n\u001b[1;32m     16\u001b[0m             pp.trainTrees[i*batch_size:(i+1)*batch_size])\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnumHits\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mhits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = TNN(pp)\n",
    "batch_size = 128\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9)\n",
    "# optimizer = optim.Adam(model.parameters(),lr=1e-3)\n",
    "epochs = 200\n",
    "printInterval=1\n",
    "losses = []\n",
    "for epoch in range(200):\n",
    "    random.shuffle(pp.trainTrees)\n",
    "    running_loss,numExamples,numHits=0,0,0\n",
    "    trainSize = len(pp.trainTrees)\n",
    "    steps = trainSize//batch_size\n",
    "    for i in tqdm(range(steps)):\n",
    "        optimizer.zero_grad()\n",
    "        _,loss,hits,cases=model(\\\n",
    "            pp.trainTrees[i*batch_size:(i+1)*batch_size])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        numHits+=hits\n",
    "        numExamples+=cases\n",
    "        running_loss+=loss.item()\n",
    "    if epoch%printInterval==0:\n",
    "        print(\"Epoch {} training loss: {:.4f}\".format(epoch,\n",
    "                    running_loss/numExamples))\n",
    "        print(\"Epoch {} training accuracy: {:.4f}\".format(epoch,\n",
    "                    numHits/numExamples))\n",
    "        losses.append(running_loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
